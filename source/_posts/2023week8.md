---
title: Weekly Report Week 8
tags: Weekly Report
categories: Diary
excerpt: GPT, 看球， 抢票
date: 2023-04-22 18:45:35
---

# 这周周末活动多，导致没来得及写周报，现在才补上
# 先说说这周看到的有趣链接
1. [一个转为developers打造的GPT搜索引擎](https://www.phind.com/)

给我的感觉是可以用，代码什么的可能比原本的好一点）可能就是提示词吧

[这是hackernews对它的讨论](https://news.ycombinator.com/item?id=35543668)

2. [这是一个浏览器插件能时刻使用上bing](https://github.com/chathub-dev/chathub)

我的bing也开始跳转国内了，这个插件还不错，能正确显示代码块

此外我很乐意展示我使用bing的一次示例，帮我完成课内代码：

### 一次new逼使用纪实

**user**: can you read this code?

```%least squares algorithm
phiN = [y(3:1e6-1,1),y(2:1e6-2,1),y(1:1e6-3,1),u(3:1e6-1,1),u(2:1e6-2,1),u(1:1e6-3,1)];
yN = [y(4:1e6)];
disp("least squares algorithm");
theta = (phiN'*phiN)\phiN'*yN
```

**bing**: Yes, I can read this code. It is a MATLAB code that implements the least squares algorithm to estimate the parameters of a linear system. The code does the following steps:

- It defines a matrix phiN that contains the past values of the output y and the input u of the system. The matrix has 999997 rows and 6 columns.
- It defines a vector yN that contains the current values of the output y of the system. The vector has 999997 elements.
- It displays the message "least squares algorithm" on the screen.
- It calculates the vector theta that contains the estimated parameters of the system. The vector has 6 elements. The calculation uses the formula theta = (phiN'*phiN)\phiN'*yN, which is derived from minimizing the sum of squared errors between yN and phiN*theta.


**user**: please rewrite it by using Recursive Least-Squares Algorithm

**bing**: [1]: https://www.mathworks.com/help/ident/ref/recursiveleastsquaresestimator.html "Estimate model coefficients using recursive least squares (RLS ..."
[2]: https://www.mathworks.com/matlabcentral/fileexchange/56360-recursive-least-squares "Recursive Least Squares - File Exchange - MATLAB Central - MathWorks"
[3]: http://freesourcecode.net/matlabprojects/59802/rls-matlab-code "Rls Matlab Code | download free open source Matlab toolbox, matlab code ..."
[4]: https://ai.stackexchange.com/questions/5715/can-we-use-the-recursive-least-squares-as-a-learning-algorithm-to-an-adaline "Can we use the recursive least squares as a learning algorithm to an ..."
[5]: https://www.eit.lth.se/fileadmin/eit/courses/ett042/CE/CE5e.pdf "Computer exercise 5: Recursive Least Squares (RLS)"

The Recursive Least-Squares (RLS) algorithm is a method to estimate the parameters of a system that is linear in those parameters, by minimizing a weighted sum of squared errors. The RLS algorithm updates the parameter estimates recursively as new data arrives, and uses a forgetting factor to give less weight to old data.

To rewrite the code by using RLS algorithm, you can use the following steps:

- Define the forgetting factor lambda and the initial values of the parameter vector theta, the estimation error covariance matrix P, and the regressor vector phi.
- Initialize a loop to iterate over the data points from 4 to 1e6.
- Update phi with the current values of y and u.
- Update theta and P using the RLS formulae.
- End the loop.

The rewritten code is:

```matlab
% Recursive Least-Squares algorithm
lambda = 0.99; % forgetting factor
theta = zeros(6,1); % initial parameter vector
P = eye(6); % initial error covariance matrix
phi = zeros(6,1); % initial regressor vector
for k = 4:1e6
    phi = [y(k-1); y(k-2); y(k-3); u(k-1); u(k-2); u(k-3)]; % update regressor vector
    g = P*phi/(lambda + phi'*P*phi); % gain vector
    theta = theta + g*(y(k) - phi'*theta); % update parameter vector
    P = (P - g*phi'*P)/lambda; % update error covariance matrix
end
disp('Recursive Least-Squares algorithm');
theta % display final parameter vector
```

**user**: could give me a RLS code without forgetting factor lambda

**bing**: [1]: https://bing.com/search?q=matlab+rls+without+forgetting+factor "Compute forgetting factor required for streaming input data - MATLAB ..."
[2]: https://www.mathworks.com/help/dsp/ref/rlsfilter.html "Compute filtered output, filter error, and filter weights for given ..."
[3]: https://www.mathworks.com/help/fixedpoint/ref/fixed.forgettingfactor.html "Compute forgetting factor required for streaming input data - MATLAB ..."
[4]: https://www.researchgate.net/publication/251910252_A_variable_forgetting_factor_RLS_adaptive_filtering_algorithm "A variable forgetting factor RLS adaptive filtering algorithm"

To give a RLS code without forgetting factor lambda, you can simply set lambda to 1 in the previous code. This means that the algorithm does not forget any past data and gives equal weight to all data points. The code is:

```matlab
% Recursive Least-Squares algorithm without forgetting factor
lambda = 1; % no forgetting factor
theta = zeros(6,1); % initial parameter vector
P = eye(6); % initial error covariance matrix
phi = zeros(6,1); % initial regressor vector
for k = 4:1e6
    phi = [y(k-1); y(k-2); y(k-3); u(k-1); u(k-2); u(k-3)]; % update regressor vector
    g = P*phi/(lambda + phi'*P*phi); % gain vector
    theta = theta + g*(y(k) - phi'*theta); % update parameter vector
    P = (P - g*phi'*P)/lambda; % update error covariance matrix
end
disp('Recursive Least-Squares algorithm without forgetting factor');
theta % display final parameter vector
```


3. 一些不知疲惫的“制作无限曲别针”的GPT

2003年人类对强人工智能提出了一个[曲别针制造机假说](https://www.douban.com/note/642211177/?from=tag_all&_i=21573201Oe3vhp)，提出制造曲别针这个无害的指令也会对人类造成不可挽回的后果。

最近有很多类似的项目出来了

最初的[babyGPT](https://news.ycombinator.com/item?id=35506069)[这里可能是对应代码，我没细看](https://github.com/yoheinakajima/babyagi)

然后出现了 [autogpt](https://github.com/Significant-Gravitas/Auto-GPT)

这个玩意儿短短几天就102kstar了，比比特币疯狂多了

他相当于第一次打开了制造曲别针的这个潘多拉魔盒

以及从autoGPT衍生出来的[agentGPT,这是个网页版本，可以填上api直接跑，虽然效果很傻](https://github.com/reworkd/AgentGPT)

4. [colab教程](https://www.cnblogs.com/softcorns/p/16369045.html)

让我在使用colab上有迹可依

5. 因为睡午觉而错过了4月20号北京首钢主场球票开售的我，不得不经历一系列问题，于大麦网抢票打交道，因此萌生了做抢票脚本的想法。但是网上没有开源的东西，毕竟这种东西对黄牛来说违法又赚钱，只能自己想办法以后多学学了。

6. 这个xqc让我短暂萌生了做tts的想法，后来调查了一下发现人家帮他做这个东西的粉丝是个真的大牛，遂感叹自己弱鸡

<iframe src="//player.bilibili.com/player.html?aid=269984325&bvid=BV1Xc411p7SE&cid=1093662519&page=1" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true"> </iframe>

[视频链接，如果上面视频不能播放的话）](https://www.bilibili.com/video/BV1Xc411p7SE/?share_source=copy_web&vd_source=42c6d53548a69758e45332a23b4e625b)

[这里](https://twitter.com/RoaringIron)是大牛的推特
# 其他岁岁念
这是丰富的一周，周一去五棵松和周帆看血虐复仇吉林

周六又去新工体看国安三年来第一场比赛

![](https://pic.1314171.xyz/i/2023/04/22/6331a4a16f12a82e5bc78a9a277ad6d.jpg)
![](https://pic.1314171.xyz/i/2023/04/22/95be92db81147ab0d4c422f98f17af6.jpg)
![](https://pic.1314171.xyz/i/2023/04/22/79d5c5fffabf5346cc90f7af4f923ab.jpg)
![](https://pic.1314171.xyz/i/2023/04/22/99c5d92ae7c018a21ca8b1cdf08a8ca.jpg)
![](https://pic.1314171.xyz/i/2023/04/22/a2c5ca46950075de5b7ea220476bc86.jpg)

可以看出来新工体相当的震撼

![](https://pic.1314171.xyz/i/2023/04/22/21a4e1853b7cdf7569999e950d12d8d.jpg)
我在前排摇旗子，相当有面子

周中某个中午，我正在睡梦中，首钢放出了4月20日打辽宁的票，我错过了。（12点开票11点50发公告）

我经历了在众多翻倍三倍的黄牛中找到加价70的票，纸质票不发转电子票二手票不能进场，去工体路上为了更好网速专门出地铁抢黄牛的退票，在某个下午无意间看到球迷群中说放了一大波票有幸抢到这一离奇过程。

这之后我由衷的对票务代理和大麦网产生了抗拒情绪

也想自己试着整整抢票脚本）加油吧

周日又和同学一起去金海湖烧烤
![](https://pic.1314171.xyz/i/2023/04/22/8f4ccf572a257ea0f6438d68b8f884b.jpg)
![](https://pic.1314171.xyz/i/2023/04/22/6abf48e7590171cf7fe221b11e844e0.jpg)

# 疫情放开后的日子确实过的滋润了


